{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ace6a3",
   "metadata": {},
   "source": [
    "Incomplete:\n",
    "- [ ] Exercise 2 - Almost done, but there are gaps\n",
    "- [x] Exercise 8b\n",
    "- [x] Exercise 3\n",
    "- [ ] Spell check each question\n",
    "- [ ] Check code comments\n",
    "- [ ] Ctrl+f for brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74078d04",
   "metadata": {},
   "source": [
    "# Machine Learning - 2<sup>st</sup> Assignment\n",
    "\n",
    "## Student: Sheedeh Sharif Bakhtiar\n",
    "### Student ID: 400422108\n",
    "\n",
    "> Code + data can also be found [here on my GitHub](https://github.com/SSBakh07/Statistical-ML---Spring-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476305ff",
   "metadata": {},
   "source": [
    "# **Exercise 1:**\n",
    "\n",
    "## *Is it possible for an SVM classifier to provide a confidence score or probability when making predictions on a particular instance? Explain it.*\n",
    "\n",
    "**Support vector machines (SVMs)**, also known as *support vector networks* are supervised machine learning models used in both regression and classification tasks. Concisely put, they work by finding a hyperplane in an $n$-dimensional space (with $n$ being the number of features) that (hopefully) separate our data points such that data points belonging to one class reside on one side of the hyperplane and data points that belong to the other class reside on the other side of the hyperplane. In other words, this hyperplane is defined such that where a specific data point resides relative to the hyperplane denotes what class this point belongs to.\n",
    "\n",
    "Mathematically, the hyperplane is defined as follows:\n",
    "\n",
    "$$\n",
    "f(X) = \\beta_0 + ∑^{N}_{i=1}\\beta_iX_i\n",
    "$$\n",
    "\n",
    "Where $\\beta_0$ is our intercept, and $\\beta_i$ are the coefficients that define where our hyperplane lies in $n$ dimensional space. We try to define our hyperplane such that we maximize the margin between data points of different classes.\n",
    "\n",
    "Whether $f(X_i)>0$ hold for a specific sample $X_i$ would dictate what class this data sample belongs to. However, it is important to note that usually, our data isn't so easily separable such that we can find a hyperplane that cleanly splits our space into two regions, so something known as a *soft margin*, where we can allow points to be misclassified to a certain degree. However, this is beyond the scope of this question.\n",
    "\n",
    "In general, SVMs are better suited when the classes are relatively separate, otherwise logistic regression and SVMs have similar performance.\n",
    "\n",
    "However, it should be noted that the SVMs described above are inherently binary classifiers, as the hyperplane divides the space into two regions. In multi-class scenarios, we can choose one of two approaches:\n",
    "1. **One-vs-One (OvO):** In this approach, we train pair-wise classifiers for each $\\binom{k}{2}$ class combinations for all $k$ classes, and for each new data point we wish to classify, we pick the class that wins the most pairwise battles between all the other classes. This solution is more preferred when the number of classes is low.\n",
    "2. **One-vs-Rest (OvR):** In this approach, we train one classifier for each of the $k$ classes. Each of these $k$ classes serve to classify points of one class against the rest of the classes in a binary classification task. The class with the classifier that has the largest value of $\\hat{f_k}(x^{*})$ (e.g. class corresponding to classifier that denotes the point to have the most distance from the hyperplane in the positive region) is considered to be the data point's class.\n",
    "\n",
    "However, now that I've outlined how SVMs work, we can tackle whether confidence scores are possible when utilizing SVMs for a specific classification task. \n",
    "\n",
    "A method for calculating confidence scores would be to take the data point's distance from the hyperplane (e.g. $\\left | \\hat{f_k}(x^{*}) \\right |$) into account. In other words, our SVM score would directly correlate to our classifier's *confidence*.\n",
    "\n",
    "However, as SVMs are, it is impossible to obtain probabilities as SVMs by nature are *not* probabilistic. However, we can introduce probabilities into our classifier. [As referenced in a Cross Validated question](https://stats.stackexchange.com/questions/55072/svm-confidence-according-to-distance-from-hyperline), [Platt (1999)](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=BFDD4EA8843BCB150DD396F11F01075E?doi=10.1.1.41.1639&rep=rep1&type=pdf) describes a method for obtaining probabilities, as it appears that class conditional densities between margins are exponential. Platt describes a sigmoid model suggested by Bayes' rule as such:\n",
    "\n",
    "$$\n",
    "P(y=1 | f) = \\frac{1}{1 + exp(Af + B))}\n",
    "$$\n",
    "\n",
    "Which is equivalent to assuming the output of the SVM is proportional to the log-likelihood of a positive training example. $A$ and $B$ are parameters that can be found. \n",
    "\n",
    "In conclusion, SVMs do not output probabilities, but we can add elements that can help us transform distance to hyperplane to probabilities.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Wikipedia: Support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "- [Towards Data Science: Support Vector Machine — Introduction to Machine Learning Algorithms](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n",
    "- [Tibshirani's Slides on Statistical Learning](https://scheshmi.github.io/CS-SBU-MachineLearning-2023/resources/slides) \n",
    "- [Prateek Joshi: How To Compute Confidence Measure For SVM Classifiers](https://prateekvjoshi.com/2015/12/15/how-to-compute-confidence-measure-for-svm-classifiers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42f11a",
   "metadata": {},
   "source": [
    "<a id=\"ex2\"></a>\n",
    "\n",
    "# **Exercise 2:**\n",
    "\n",
    "## *What actions should you take if you have trained an SVM classifier using an RBF kernel but notice that it underfits the training set? Would it be appropriate to increase or decrease the value of γ (gamma) or C, or both?*\n",
    "\n",
    "\n",
    "\n",
    "Before I answer this question, I'll first briefly explain what $γ$ and $C$ are in the context of support vector machines;\n",
    "\n",
    "$C$ is a regularization parameter that denotes how tolerant we want our SVM to be towards misclassifications. In other words, $C$ denotes how much we penalize our classifier for misclassifying each point. The larger $C$ is, the smaller our margin is, and the smaller $C$ is, the larger our margin is. Increasing $C$ until there are no more improvements may not be a good idea through, because a small margin means that we may risk overfitting on the training set. Similarly, small values for $C$ may also be bad, as if $C$ is small enough, you might get misclassified examples even if the training data is linearly separable. Similar to learning rates in other machine learning models, a balance is needed.\n",
    "\n",
    "$γ$ is a hyperparameter used in non-linear SVMs, such as SVMs that use non-linear kernels such as RBF. The RBF kernel is defined as below:\n",
    "\n",
    "$$\n",
    "K(X_1, X_2) = exp(- \\frac{\\left \\| X_1 - X_2 \\right \\|^2}{2\\sigma^2})\n",
    "$$\n",
    "\n",
    "Where $\\sigma$ is the variance and []\n",
    "\n",
    "In non-mathematical terms, the RBF kernel function for two points $X_1$ and $X_2$ compute similarity or how close they are to each other.\n",
    "\n",
    "When tuning linear SVMs, we only need to worry about one parameter; $C$. However, should we be using a kernel such as RBF, not only do we have two parameters ($C$ *and* $γ$) but these two influence each other. If $γ$ is large, than the influence of $C$ becomes negligible. If $γ$ is too small, then $C$ alters the model just like it would on a linear model.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Medium: What is the Significance of C value in Support Vector Machine?](https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a)\n",
    "- [Cross Validated: What is the influence of C in SVMs with linear kernel?](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)\n",
    "- [Towards Data Science: SVM Hyperparameters Explained with Visualizations](https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b#:~:text=Gamma%20is%20a%20hyperparameter%20used,of%20a%20single%20training%20point.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c54a9",
   "metadata": {},
   "source": [
    "\n",
    "# **Exercise 3:**\n",
    "\n",
    "## *What does it mean for a model to be ϵ-insensitive?*\n",
    "\n",
    "Epsilon insensitive means that in our model's cost function, there is a margin of error that is ignored such that should a misclassified data point fall within the range of $|\\epsilon|$ around our decision boundary, our loss function will choose to ignore that misclassification. By defining an error-insensitive margin like this, we can avoid overfitting to data points that lie very very closely to the decision boundary.\n",
    "\n",
    "$\\epsilon$ can be thought of as a tube around our decision boundary (e.g. for an SVM it would be a hyperplane, such that should an error fall *within* that tube, it'll be ignored. Hence, the phrase \"$\\epsilon$-insensitive\". This concept is not unique to SVMs, and can be used in other models as well.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Andrew Zisserman's Machine Learning Slides on Regression](https://www.robots.ox.ac.uk/~az/lectures/ml/2011/lect6.pdf)\n",
    "- [Towards Data Science: Understanding SVR and Epsilon Insensitive Loss with Scikit-learn](https://towardsdatascience.com/understanding-svr-and-epsilon-insensitive-loss-with-scikit-learn-28ec03a3d0d9)\n",
    "- [Cross Validated: Meaning of Epsilon in SVM regression](https://stats.stackexchange.com/questions/259018/meaning-of-epsilon-in-svm-regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7ddb5",
   "metadata": {},
   "source": [
    "# **Exercise 4:**\n",
    "## *What is the difference between hard margin and soft margin SVM? When would you use each one?*\n",
    "\n",
    "\n",
    "As briefly mentioned in previous questions, SVMs work by finding a hyperplane in $m$ dimensional feature space that best separates the data into two parts. The class label associated with each point depends on where said point lies relative to this hyperplane.\n",
    "\n",
    "The distance between each data point close to the hyperplane and the hyperplane itself is known as the *margin*. SVM is looking to maximize this margin as much as possible.\n",
    "\n",
    "In general, we have two types of margins: *hard* margins and *soft* margins.\n",
    "\n",
    "*Hard margins* are used when we don't want any misclassifications. This means that our hyperplane needs to be defined such that each point of each class lies on their side of the space divided by said hyperplane. Obviously, this is only possible when our data is linearly separable.\n",
    "\n",
    "> For data to be linearly separable, there needs to exist a hyperplane where *all* points of one class lie on one side of the hyperplane and *all* points of the other class lie on the other. In other words, we're able to neatly separate data from different classes from one another.\n",
    "\n",
    "*Soft margins* are used when the data is not linearly separable. In these cases, we need to define our margin such that errors are allowed to some capacity, because we're not able to separate our data so neatly. However, this new hyperplane that allows for misclassifications must be defined such that misclassification error is at a minimum, and the margin for points closest to the hyperplane is at a maximum. This trade-off between margin and misclassification is done using the parameter $C$, which dictates how much we wish to penalize our model for each misclassification. The exact details are discussed in [Exercise 2](#ex2).\n",
    "\n",
    "However it should be noted that sometimes while our data might be linearly separable, but a soft margin would *still* be preferable. This is due to the fact that if we were to use a hard margin, we'd risk having a very small margin, where we'd risk overfitting, or we'd be far too sensitive to outliers that lie too close to said hard margin. So, we might want to prefer using a soft margin where we'd want to make sure our model is generalizing.\n",
    "\n",
    "In summary, we might want to use hard margins when our data is linearly separable or we're looking to minimize misclassifications as much as possible. When our data is *not* linearly separable, and/or we're looking to prevent any potential overfitting, a soft margin would be more preferable.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Baeldung: Using a Hard Margin vs. Soft Margin in SVM](https://www.baeldung.com/cs/svm-hard-margin-vs-soft-margin#:~:text=The%20difference%20between%20a%20hard,be%20feasible%20to%20do%20that.)\n",
    "- [Section: Using a Hard Margin vs Soft Margin in Support Vector Machines](https://www.section.io/engineering-education/using-a-hard-margin-vs-soft-margin-in-support-vector-machines/)\n",
    "- [Analytics Vidhya: Introduction to SVM](https://www.analyticsvidhya.com/blog/2021/04/insight-into-svm-support-vector-machine-along-with-code/)\n",
    "- [Wikipedia: Linear Separability](https://en.wikipedia.org/wiki/Linear_separability)\n",
    "- [StackOverFlow: SVM - hard or soft margins?](https://stackoverflow.com/questions/4629505/svm-hard-or-soft-margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3270c695",
   "metadata": {},
   "source": [
    "# **Exercise 5:**\n",
    "## *Is a node’s Gini impurity generally lower or greater than its parent’s? Is it generally lower/greater, or always lower/greater?*\n",
    "\n",
    "During the decision tree learning process, at each split we are looking for the \"decision\" that will best fulfill some sort of condition based on some metric. One of the metrics used to makes said decisions, is the Gini impurity measure, which we are looking to decrease at every step.\n",
    "\n",
    "The Gini impurity measure is defined as such: \n",
    "\n",
    "If we have a total of $C$ classes and $p(i)$ is the probability of picking a datapoint with class $i$, then the Gini impurity is calculated as such:\n",
    "\n",
    "$$\n",
    "G = \\sum^{C}_{i=1}p(i) * (1 - p(i))\n",
    "$$\n",
    "\n",
    "Intuitively, this metric simply gives us a hint as to what the probability of misclassifying an observation is, thus at each step we are looking to decrease it. The lower our Gini impurity is at each step, the better, because this decreases the probability of misclassification. We make splits that give us the most possible *Gini gain*, meaning the highest difference in Gini impurity before and after the split.\n",
    "\n",
    "Therefore, the Gini impurity of a child node is always decreasing, otherwise, a split just wouldn't be made.\n",
    "\n",
    "- [Towards Data Science: ](https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33)\n",
    "- [Victor Zhou: A Simple Explanation of Gini Impurity](https://victorzhou.com/blog/gini-impurity/)\n",
    "- [Wikipedia: Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning#:~:text=Gini%20impurity%20measures%20how%20often,into%20a%20single%20target%20category.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68c402",
   "metadata": {},
   "source": [
    "# **Exercise 6:**\n",
    "## *Is it a good idea to consider scaling the input features if a Decision Tree underfits the training set?*\n",
    "\n",
    "In general, due to how decision trees work by trying to decrease Gini impurity at each step, this specific algorithm (and by extension, decision tree-derived algorithms such as [random forest](https://en.wikipedia.org/wiki/Random_forest)) is not sensitive to feature scaling or any sort of normalization. This is because the split on a feature is not influenced by other features.\n",
    "\n",
    "Therefore, if our decision tree model is underfitting, features scaling will not have much of an effect, however, hyperparameter tuning such as increasing the number of splits/increasing the depth of the tree, or utilizing ensemble methods may help alleviate the problem.\n",
    "\n",
    "*Sources:*\n",
    "- [atoti: When to perform Feature Scaling?](https://www.atoti.io/articles/when-to-perform-a-feature-scaling/#:~:text=The%20decision%20tree%20splits%20a,the%20scale%20of%20the%20features.)\n",
    "- [Forcastegy: Do Decision Trees Need Feature Scaling Or Normalization?](https://forecastegy.com/posts/do-decision-trees-need-feature-scaling-or-normalization/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce457a5",
   "metadata": {},
   "source": [
    "# **Exercise 7:**\n",
    "## *How can you use tree-based models for feature selection?*\n",
    "\n",
    "We can calculate feature importance by fitting our tree to our data. In general, at every level splits are made based on gini impurity decrease, and the more important a feature is at a certain split, the greater the impurity decrease is. \n",
    "\n",
    "We can determine feature importance by averaging the impurity decrease per-feature across all splits, resulting in an approximate ranking of features based on importance.\n",
    "\n",
    "However, it should be noted that the metric used during splitting (Gini impurity or otherwise) influences the potential ranking of variables, so we should take care in choosing our splitting metric wisely.\n",
    "\n",
    "*Sources:*\n",
    "- [Towards Data Science: Feature Selection Using Random forest](https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f)\n",
    "- [Busigence: Feature selection using Decision Tree](https://busigence.com/blog/feature-selection-using-decision-tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa6c06",
   "metadata": {},
   "source": [
    "# **Exercise 8:**\n",
    "## *How do you tweak the hyperparameters of the following model in mentioned circumstances:*\n",
    "### *AdaBoost - Underfitting*\n",
    "\n",
    "AdaBoost — as the name suggests, is a popular boosting algorithm which takes several weak learners to combine them into a single strong learner.\n",
    "\n",
    "> [Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning)) is a family of ensemble meta-learners with the motivation of creating a strong learner by grouping a number of weak learners to work together. By grouping a number of different predictors together, we can reduce the bias in our model while keeping variance fixed.\n",
    "\n",
    "The most common estimator used in AdaBoost are decision trees with a single split, also known as a *stump*. AdaBoost ensembles have a number of different parameters (for example, [scikit-learn's documentation on `sklearn.ensembleAdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)), but should we be faced with a situation where our AdaBoost model is underfitting, we can attempt to alleviate the problem by increasing the number of weak learners in our ensemble.\n",
    "\n",
    "\n",
    "### *Gradient Boosting - Overfitting*\n",
    "\n",
    "Gradient Boosting, much like AdaBoost, is an ensemble method where several weak learners come together to make a single strong learner. The difference between these two is in how these weak learners are created during the iterative process. For one, Gradient boosting methods train on error residuals, while AdaBoost does not.\n",
    "\n",
    "If our Gradient Boosting model is overfitting, we can reduce the number of weak learners, or decrease the \"strength\" of each of the weak learners.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Wikipedia: AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "- [Paperspace: A Guide to AdaBoost: Boosting To Save The Day](https://blog.paperspace.com/adaboost-optimizer/)\n",
    "- [Wikipedia: Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [Quora: What is the difference between gradient boosting and AdaBoost?](https://www.quora.com/What-is-the-difference-between-gradient-boosting-and-AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d9a80",
   "metadata": {},
   "source": [
    "# **Exercise 9:**\n",
    "## *What is the difference between homogeneous and heterogeneous ensembles? Which one is more powerful?*\n",
    "\n",
    "As briefly mentioned in previous questions, ensembles are a group of machine learning models working together to create one, single model that is stronger than each of the individual models. \n",
    "\n",
    "In general, we can divide ensemble models into two different categories: \n",
    "\n",
    "- *Homogenous* ensembles which are a collection of classifiers all with the same type (e.g. all of the classifiers are decision trees, or logistic regression models) but built on different subsets of the data. An example of this would be random forest classifiers, where all classifiers are decision trees, but each tree is trained on different data. These algorithms should not be fine-tunes and should be left weak. This way, we can reduce the risk of over-fitting.\n",
    "\n",
    "- *Heterogenous* ensembles are a collection of classifiers with different types, but each have a different types. For instance, an ensemble of logistic regressors, decision trees and SVMs all trained on the same data would be considered an ensemble of this type.\n",
    "\n",
    "While homogenous models are incredibly powerful, they also need a lot of data. Similarly, heterogenous ensembles can be quite powerful as well, but depending on the models used in the ensemble, they might be a bit computationally expensive to train.\n",
    "\n",
    "*Sources:*\n",
    "- [Analytics Vidhya: Exploring Ensemble Learning in Machine Learning World!](https://www.analyticsvidhya.com/blog/2021/01/exploring-ensemble-learning-in-machine-learning-world/#:~:text=HOMOGENEOUS%20ENSEMBLE%20is%20a%20collection,in%20the%20Random%20Forest%20model.&text=HETEROGENEOUS%20ENSEMBLE%20is%20a%20set,built%20upon%20the%20same%20data.)\n",
    "- [Stack OverFlow: Homogeneous vs heterogeneous ensembles](https://stackoverflow.com/questions/49445446/homogeneous-vs-heterogeneous-ensembles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd3e1c",
   "metadata": {},
   "source": [
    "# **Exercise 10:**\n",
    "## *How ROC and AUC are being used in the evaluation of classification performance?*\n",
    "\n",
    "A *receiver operating characteristic curve* (better known as an *ROC curve*) is a graph which shows the performance of a classification model at all classification threshold. This curve plots two parameters against each other; the true positive rate and the false positive rate.\n",
    "\n",
    "> The true positive rate (also known as *recall* is a reflection of the number of predictions the classifier has *correctly* classified as part of a specific class and the false positive rate is a reflection of the number of predictions that the classifier has flagged as a false positive.\n",
    "\n",
    "*AUC* (which stands for *\"Area under the ROC curve\"*) measures the area underneath the ROC curve for the purpose of providing an aggregate measure of performance across all possible classification thresholds.\n",
    "\n",
    "The value of AUC dictates the measure of separability in a model, in other words, how well our model can separate the classes from one another. Optimally, we'd want our AUC to be 1, meaning that it's always able to separate the classes. When AUC is at 0.5, our model is no better than a random model, which is considered to be the worst case scenario. In other words, AUC gives us an estimated probability of classifying a data point correctly (e.g. an AUC of 0.7 would mean we have 70% chance of classifying a point correctly).\n",
    "\n",
    "In summary, the AUC is obtained from the ROC, and using the AUC we can gauge how well our classifier is performing given metrics such as the true positive rate and false positive rate.\n",
    "\n",
    "In a multiclass scenario, we could plot the classes against each other in a one-vs-all scenario, thus extending this metric to a multiclass setting.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Google: Classification: ROC Curve and AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=AUC%20stands%20for%20%22Area%20under,across%20all%20possible%20classification%20thresholds.)\n",
    "- [Towards Data Science: Understanding AUC - ROC Curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e07d1c",
   "metadata": {},
   "source": [
    "# **Exercise 11:**\n",
    "## *How does the threshold value used in classification affect the model's performance? This value specifies a cut-off for an observation to be classified as either 0 or 1. Can you explain the trade-off between false positive and false negative rates, and how the choice of threshold value impacts precision and recall?*\n",
    "\n",
    "Many classification models do not output 0s and 1, but instead output confidences, or the probability that a certain data sample belongs to a certain class. To convert these continuous values into a discrete 0 and 1 for our final prediction, we define a *threshold value*, at which we divide our model output into two parts; anything less than this threshold value would be considered a 0, and anything more than equal to this threshold value would be considered a 1.\n",
    "\n",
    "While by default, most model's threshold value is set to 0.5, this might not always be the best choice. In fact, we need to take care in how we pick our threshold value, as this specific value is problem-specific and should be fine tuned.\n",
    "\n",
    "For instance, in some problems we might be sensitive towards false positives (for example, it's better to erronously flag a patient's test results as potentially bad as opposed to accidentally classifying a sick patient as healthy) and in some problems we may want to avoid false negatives more. Therefore, the specific trade-off between false positives and false negatives is completely domain-specific.\n",
    "\n",
    "One of the best ways to pick our threshold value is by utilizing ROC curves, which were discussed in the previous exercise, or a precision-recall curve.\n",
    "\n",
    "> The ratio of true positives to the sum of true positives and false positives is called *precision*. As the name suggests, precision indicates the model's *precision*, as in, how confident we can be in our prediction when our model assumes a data sample is positive (e.g. our model output is 1).\n",
    "\n",
    "A precision-recall curve solely focuses on the performance of the classifier on the positive class, so if we are looking to fine-tune our model in favor of having greater precision (e.g. being more confident when it comes to positive predictions) or greater recall (e.g. being more likely to flag a data sample as positive), we can use this specific graph.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [DeepChecks: What is a Threshold in Machine Learning](https://deepchecks.com/glossary/classification-threshold/#:~:text=A%20classification%20threshold%20value%20must,always%20going%20to%20be%200.5%E2%80%A6)\n",
    "- [iguazio: What is the Classification Threshold in Machine Learning?](https://www.iguazio.com/glossary/classification-threshold/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fb669",
   "metadata": {},
   "source": [
    "# **Exercise 12:**\n",
    "## *What is the difference between one-vs-one and one-vs-all multiclass classification approaches in classifiers? Under what circumstances would you use one over the other?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
