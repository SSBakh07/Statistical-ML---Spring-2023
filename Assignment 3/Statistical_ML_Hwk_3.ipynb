{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3614485",
   "metadata": {},
   "source": [
    "# Machine Learning - 3<sup>rd</sup> Assignment\n",
    "\n",
    "## Student: Sheedeh Sharif Bakhtiar\n",
    "### Student ID: 400422108\n",
    "\n",
    "> Code + data can also be found [here on my GitHub](https://github.com/SSBakh07/Statistical-ML---Spring-2023)\n",
    "\n",
    "\n",
    "# **Exercise 1:**\n",
    "## *What is the curse of dimensionality and how does it affect clustering?*\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality) the \"curse of dimensionality\" refers to the various \"hurdles\" we may face when analyzing data in high-dimensional spaces that do not occur in low-dimensional spaces, such as three-dimensional physics space of everyday experience. The curse of dimensionality can result in an exponential increase in computational efforts requried for processing and analizying data.\n",
    "\n",
    "This specific \"curse\" might cause many problems in fields such as machine learning, as the more dimensions there are, the \"further\" apart various data points might be from one another, given the extra dimensions to take into account. As the dimensionality increases, the number of data points required for good performance increases exponentially.\n",
    "\n",
    "> It has been shown in [this 1968 study by Hughes](https://www.scirp.org/(S(czeh2tfqw2orz553k1w0r45))/reference/referencespapers.aspx?referenceid=1019884) study from 1968 that increasing the number of dimensions also improves a classifier's performance *up until a certain point* — after which the classifier's performance deteriorates, so increasing the number of dimensions isn't always a *bad* idea. Therefore the curse of dimensionality is also sometimes referred to as the Hughes phenomenon.\n",
    "\n",
    "Unfortunately, the curse of dimensionality has a direct effect on distance functions. As distance functions are *vital* in clustering algorithms, as clusters tend to be defined and adjusted using these distance metrics (i.e. Manhattan distance), the curse of dimensionality and the number of dimensions plays a critical role in clustering algorithms.\n",
    "\n",
    "For example, for a given point $A$, let us assume $dist_{min}(A)$ is the distance between $A$ and it's nearest neighbor, while $dist_{max}(A)$ is the distance between $A$ and it's farthest neighbor.\n",
    "\n",
    "In low-dimensional space, i.e. one-dimensional, two-dimensional or even three-dimensional space, we have:\n",
    "\n",
    "$$\n",
    "\\frac{dist_{max(A)} - dist_{min(A)}}{dist_{min(A)}} > 0\n",
    "$$\n",
    "\n",
    "But as the dimensions increase, that is as $dim \\rightarrow \\infty$, we have:\n",
    "\n",
    "$$\n",
    "lim_{dim \\rightarrow \\infty}(\\frac{dist_{max(A)} - dist_{min(A)}}{dist_{min(A)}} > 0) \\rightarrow \\infty\n",
    "$$\n",
    "\n",
    "In other words, for a $d$-dimensional space (with $d \\rightarrow \\infty$), given $n$ random points, the $dist_{min(A)} ≈ dist_{max(A)}$, meaning that any given pair of points are equidistant to one another.\n",
    "\n",
    "Some solutions to this issue include using different distance metrics such as cosine similarity to replace Euclidean distance, or use dimensionality-reducing techniques such as PCA to help alleviate the problem.\n",
    "\n",
    "*Sources:*\n",
    "- [Wikipedia: Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "- [Towards Data Science: Curse of Dimensionality — A “Curse” to Machine Learning](https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5cde1",
   "metadata": {},
   "source": [
    "<a id=\"ex2\"></a>\n",
    "# **Exercise 2:**\n",
    "## *In what cases would you use regular PCA, incremental PCA, randomized PCA, or random projection?*\n",
    "\n",
    "Principal component analysis or PCA, is a statistical technique for reducing the number of dimensions in a dataset, thus increasing the interpretability of data while preserving the maximum amount of information and enabling the visualization of multidimensional data.\n",
    "\n",
    "PCA works by extracting the important information from the data and to express this information as a set of summary indices called principal components. In summary, PCA finds hyperplanes in the $k$ dimensional space that approximates the data as well as possible in the least squares sense. A hyperplane that is the least squares approximation of a set of data points make sthe variance of the coordinates on the line or plane as large as possible.\n",
    "\n",
    "PCA is guarenteed to produce uncorrelated features, due to *how* the new features are constructed (which is beyond the scope of this exercise) and has no hyperparameters while being relatively fast, making it easy to use.\n",
    "\n",
    "However, PCA comes with a number of drawbacks:\n",
    "- PCA should mainly be used for scenarios where variables are strongly correlated. If the relationship between variables is weak, PCA does not perform very well to reduce the number of dimensions in our dataset.\n",
    "- On a similar note, PCA assumes the relationships between features is linear. PCA may not perform very well when the relationship between datapoints is not linear.\n",
    "- PCA also requires normalization, as PCA is sensitive to scale.\n",
    "- PCA is also sensitive to outliers, and cannot handle missing values.\n",
    "- PCA is only suitable for continuous data. If our dataset consists of a mix of continuous and discrete features, perhaps another dimensionality reduction technique would be a better fit.\n",
    "\n",
    "While PCA is relatively fast and easy to use, sometimes our dataset is too large to fit into memory. **Incremental PCA** can be used in such scenarios. Incremental PCA splits the dataset into mini-batches that can fit into memory. Then, each batch is fed into our incremental PCA algorithm one at a time.\n",
    "\n",
    "Incremental PCA updates the principal components based on the new data points, without the need to recompute PCA from scratch each time, making it incredibly useful for very large datasets and at times, a more efficient and scalable approach to PCA, but it mgiht not be as accurate as computing the full PCA due to the inherent approximation of the incremental algorithm.\n",
    "\n",
    "While vanilla PCA uses low-rank matrix approximation to estimate the principal components used in dimensionality reduction, for large datasets this method could be costly and difficult to scale. Much like incremental PCA, **Randomized PCA** was designed to help solve this specific problem.\n",
    "\n",
    "Randomized PCA is a stochastic algorithm that quickly finds an approximation for the first $n$ principal components, making it dramatically faster than the previously discussed versions of PCA, as the first $n$ principal components can be approximated by computing the eigenvectors of the covariance matrix of the projected data. This makes randomized PCA significantly faster and more scalable than vanilla PCA.\n",
    "\n",
    "Similar to PCA, [random projection](https://en.wikipedia.org/wiki/Random_projection#:~:text=In%20mathematics%20and%20statistics%2C%20random,when%20compared%20to%20other%20methods.) is a statistical technique for reducing the dimensionality of a dataset. This method is known for it's power, simplicity and low error rates when compared to other methods. Random projection is significantly faster and less complex than PCA, and is robust to outliers, as opposed to PCA that was very sensitive to outliers. However, this comes at the cost of a small loss in accuracy, as PCA maintains the best possible projection. \n",
    "\n",
    "\n",
    "In summary:\n",
    "- Vanilla PCA should be used when we have a number of correlated, continuous features with no missing values as regular PCA is fast and easy to use.\n",
    "- Incremental PCA should be used when we have a dataset that's too large to fit into memory, however, this comes at the cost of slightly reduced accuracy.\n",
    "- Randomized PCA should be used when we have a dataset that's too large for vanilla PCA to handle.\n",
    "- Random projection is significantly faster than PCA methods, and is robust to outliers, so should our data contain many outliers or speed is an important factor (at the cost of a little bit of accuracy), then random projection may be the best choice for the task at hand.\n",
    "\n",
    "\n",
    "> **Additional Notes**: According to the empirical results shown in [this](https://www.kaggle.com/code/gsigalaev/incremental-pca-randomized-pca-performance-test?scriptVersionId=103870114) Kaggle notebook, incremental PCA is significantly slower than regular PCA and randomized PCA, and randomized PCA works better than regular PCA in almost all cases. However, the number of datapoints negatively impact the randomized PCA algorithm, and in cases where it's feasible, regular PCA is more feasible. \n",
    "\n",
    "*Sources:*\n",
    "- [Wikipedia: Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "- [Sartorius: What Is Principal Component Analysis (PCA) and How It Is Used?](https://www.sartorius.com/en/knowledge/science-snippets/what-is-principal-component-analysis-pca-and-how-it-is-used-507186)\n",
    "- [OriginLab: 17.7.1 Principal Component Analysis](https://www.originlab.com/doc/Origin-Help/PrincipleComp-Analysis#:~:text=PCA%20should%20be%20used%20mainly,0.3%2C%20PCA%20will%20not%20help.)\n",
    "- [Crunching the Data: When to use principal component analysis](https://crunchingthedata.com/when-to-use-principal-component-analysis/)\n",
    "- [AI Aspirant: Types of PCA](https://aiaspirant.com/types-of-pca/)\n",
    "- [OpenGenus: Sparse and Incremental PCA](https://iq.opengenus.org/sparse-and-incremental-pca/)\n",
    "- [Wikipedia: Random Projection](https://en.wikipedia.org/wiki/Random_projection#:~:text=In%20mathematics%20and%20statistics%2C%20random,when%20compared%20to%20other%20methods.)\n",
    "- [ResearchGate: Does Random Projection have advantage(s) over PCA?](https://www.researchgate.net/post/Does-Random-Projection-have-advantages-over-PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ffc10",
   "metadata": {},
   "source": [
    "# **Exercise 3:**\n",
    "## *Does it make sense to chain two different dimensionality reduction algorithms?*\n",
    "\n",
    "It does make sense to *sometimes* chain two different dimentionality reduction algorithms, as it very much depends on the data we are working with. \n",
    "\n",
    "For instance, if our dataset consists of a combination of continuous and discrete variables, we could apply PCA onto the continuous variables, and another algorithm such as MCA onto the discrete variables, thus *making up for the weaknesses of the other algorithm*. For instance, should our data exhibit non-linear relationships, we might apply one algorithm designed to capture said non-linear relationships, and another to further reduce the dimensionality.\n",
    "\n",
    "Nonetheless, we should be mindful of each algorithm's strengths and weaknesses, as combining the wrong two algorithms might introduce unwanted complexity into our data.\n",
    "\n",
    "In general, we can also chain dimensionality reduction algorithms when we have a high-dimensional dataset, and we can use one algorithm to reduce the dimensionality and then another to further reduce the dimensionality.\n",
    "\n",
    "However, it should be noted that it's usually not beneficial to chain the same dimensionality reduction algorithm twice, i.e. applying PCA twice, as there is a measure of information loss that comes with these dimensionality reduction techniques, and this information could be unbalanced across subsets of the original features. By applying PCA on an already-PCA-reduced dataset, we'll be ignoring the fact that some principal components matter more than other principal components.\n",
    "\n",
    "*Sources*\n",
    "- [Brainly: ](https://brainly.com/question/29422864)\n",
    "- [ChatGPT](https://chatgptonline.ai/chat/) (2 Prompts: `\"As a data scientist, does it make sense to chain two different dimensionality reduction algorithms together?\"` and `\"As a data scientist, when is it beneficial to chain two different dimensionality reduction algorithms together?\"`)\n",
    "- [Cross Validated: Can PCA be applied twice or more?](https://stats.stackexchange.com/questions/194005/can-pca-be-applied-twice-or-more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2401efeb",
   "metadata": {},
   "source": [
    "# **Exercise 4:**\n",
    "## *What are the main assumptions and limitations of PCA?*\n",
    "\n",
    "As explained extensively in [Exercise 2](#ex2), the main assumptions and limitations of PCA have been already listed. However, to reiterate:\n",
    "\n",
    "- PCA should mainly be used for scenarios where variables are strongly correlated. If the relationship between variables is weak, PCA does not perform very well to reduce the number of dimensions in our dataset.\n",
    "- On a similar note, PCA assumes the relationships between features is linear. PCA may not perform very well when the relationship between datapoints is not linear.\n",
    "- PCA also requires normalization, as PCA is sensitive to scale.\n",
    "- PCA is also sensitive to outliers, and cannot handle missing values.\n",
    "- PCA is only suitable for continuous data. If our dataset consists of a mix of continuous and discrete features, perhaps another dimensionality reduction technique would be a better fit.\n",
    "\n",
    "\n",
    "*Sources are the same as Exercise 2*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79c120",
   "metadata": {},
   "source": [
    "# **Exercise 5:**\n",
    "## *How can clustering be used to improve the accuracy of the linear regression model?*\n",
    "\n",
    "Linear regression is an incredibly simple method that is computationally simple and highly interpertable, but it isn't without it's drawbacks. For one, it works best when data is linearly separable, and it's unable to handle complex regions. One method for incorporating complexity in our linear regression model is to \"do some of the work beforehand\" and introduce a clustering algorithm *before* we fit our model to our data.\n",
    "\n",
    "Clustering methods such as k-means find implicit sub-groups in the data that we might not be able to detect at first glance, as clustering algorithms group data points with similar characteristics to one another.  \n",
    "\n",
    "We can \"stack\" clustering and linear regression on top of each other to create a more specific linear regression model where the variations and subgroups in the data have been identified.\n",
    "\n",
    "Additionally, we can use clustering methods to find outliers and anomalies that might've negatively impacted our model's performance.\n",
    "\n",
    "In summary, clustering methods can add additional complexity to our linear regression model and capture implicit similarities between different data points for higher accuracy.\n",
    "\n",
    "*Sources:*\n",
    "- [Cross Validated: Cluster analysis followed by regression](https://stats.stackexchange.com/questions/182744/cluster-analysis-followed-by-regression)\n",
    "- [Paperspace: Building sharp regression models with K-Means Clustering + SVR](https://blog.paperspace.com/svr-kmeans-clustering-for-regression/)\n",
    "- [ChatGPT](https://chatgptonline.ai/chat/) (1 Prompt: `\"As a data scientist, how can clustering be used to improve the accuracy of the linear regression model?\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d9ee8",
   "metadata": {},
   "source": [
    "# **Exercise 6:**\n",
    "## *How is entropy used as a clustering validation measure?*\n",
    "\n",
    "In general, the goal of clustering is to group a set of objects together such that objects in the same group are similar to one another. However, how good our clusters are is an important factor to take into account, as we'd like our clusters to be *useful* in our task at hand (such as describing our data). \n",
    "\n",
    "There are multiple metrics we can employ to measure the \"goodness\" of a cluster (otherwise known as *cluster validation*) and one of them is entropy. Entropy in a clustering context measure the probability of a record in cluster $i$ being classified as class $i$ and has the following formula:\n",
    "\n",
    "$$\n",
    "-\\sum^{K}_{i=1}p_i logp_i\n",
    "$$\n",
    "\n",
    "Entropy can be used as a method for measuring impurity. The entropy for a cluster is 0 when the probability of all data points belonging to a specific cluster is 1 — meaning that our cluster is *well-separated*. Therefore, the lower our average entropy is across our dataset, the better.\n",
    "\n",
    "Therefore, we should strive to tune our clustering algorithm (e.g. for k-means, this might mean setting the right value for $k$) that we minimize our entropy as much as possible.\n",
    "\n",
    "*Sources:*\n",
    "- [Towards Data Science: Data Clustering Tutorial for Advanced](https://towardsdatascience.com/clustering-for-data-nerds-ebbfb7ed4090)\n",
    "- [Visual Studio Magazine: Data Clustering Using Entropy Minimization](https://visualstudiomagazine.com/articles/2013/02/01/data-clustering-using-entropy-minimization.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46504def",
   "metadata": {},
   "source": [
    "# \\***Exercise 7:**\n",
    "## *What is label propagation? Why would you implement it, and how?*\n",
    "\n",
    "The label propagation algorithm (LPA) is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. We initially have a subset of datapoints that *have* labels or classifications, and these labels are then propagated to the unlabeled points using this algorithm.\n",
    "\n",
    "Another way of looking at this algorithm is as an algorithm for finding communities in a graph. LPA detects these communities using the network structure as it's only guide, and propagates labels throughout the network.\n",
    "\n",
    "Intuitively, it's *similar* to training a machine learning algorithm *that has learned a pattern in our data* onto training data and then predicting labels for test data, and assigning the predicted labels on the test data as \"truth\" and appending them to the training data. *Or*, it can be seen as a single label quickly becoming dominant in a densely connected group of nodes, but will have trouble crossing a sparsely connected region, with labels getting trapped inside a densely connected group of nodes.\n",
    "\n",
    "[According to Wikipedia](https://en.wikipedia.org/wiki/Label_propagation_algorithm), this process consists of 5 steps, which are as follows:\n",
    "\n",
    "1. Initialize all the labels in the netowrk. For a given node $x$, $C_x(0)=x$\n",
    "2. Set our time to it's first step. I.e. set $t=1$\n",
    "3. Arrange all the node in the network in a random order and set it to $X$.\n",
    "4. For each $x \\in X$ chosen in that specific order, let the label be the most frequent label occuring among the neighbors, i.e. $C_x(t) = f(C_{x_{i1}}(t), \\ldots, C_{x_{im}}(t), C_{x_{i(m+1)}}(t-1), \\ldots, C_{x_{ik}}(t-1)$. Select a label at random if there are multiple highest frequency labels.\n",
    "5. If every node has a label that the maximum number of their neighbours have, then stop the algorithm. Else, set $t=t+1$ and go to (3)\n",
    "\n",
    "How the labels are initially set is where this algorithm becomes semi-supervised as opposed to completely unsupervised. If we set the initial labels of the data points that have labels to their labels and propagate these labels to the data points that do not have labels, then we'd have labeled the unlabeled data points successfully.\n",
    "\n",
    "How we pick the nearest neighbors could be done through distance metrics, such as Euclidean distance. Implementing this algorithm is as simple as iterating through each unlabeled data point, finding it's nearest neighbors, and assigning the most frequent label to this data point, in a method that's incredibly similar to the more-familiar KNN (k-nearest neighbours) algorithm.\n",
    "\n",
    "\n",
    "*Sources:*\n",
    "- [Wikipedia: Label propogation algorithm](https://en.wikipedia.org/wiki/Label_propagation_algorithm)\n",
    "- [neo4j: Label Propagation](https://neo4j.com/docs/graph-data-science/current/algorithms/label-propagation/)\n",
    "- [Alibaba Cloud: Label propagation](https://www.alibabacloud.com/tech-news/algorithm/4l1w5hzp6am-label-propagation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270e33eb",
   "metadata": {},
   "source": [
    "# **Exercise 8:**\n",
    "## *You are going to work on the [Supermarket dataset](https://www.kaggle.com/datasets/hunter0007/ecommerce-dataset-for-predictive-marketing-2023) for predictive marketing . Your task is to use clustering algorithms to segment the customers into distinct groups based on their shopping behavior and demographics.*\n",
    "\n",
    "### *Explore and preprocess the dataset. This may involve handling categorical variables and normalizing or scaling numerical features and feature engineering.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dadd463",
   "metadata": {},
   "source": [
    "#### EDA\n",
    "\n",
    "##### Average number of orders per customer\n",
    "\n",
    "If we plot 200 customer's max order number, we'll get something like this:\n",
    "\n",
    "<br />\n",
    "\n",
    "![scatter1](eda1.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "This graph tells us that most customers order less than 20 times, and the data isn't _significantly_ varied, but there aren't any clear patterns either.\n",
    "\n",
    "##### Time dependency\n",
    "\n",
    "Next, we'll check the order frequency by time of day and day of week to see if there are any clear correlations there.\n",
    "\n",
    "First we'll check the hour of the day:\n",
    "\n",
    "<br />\n",
    "\n",
    "![barplot1](eda2.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "As you can see, most orders are in the middle of in the middle of the day, between 9 AM and 5 PM, with very few orders in the middle of the night, such as 3 or 4 AM. Next, we'll check the day of the week;\n",
    "\n",
    "<br />\n",
    "\n",
    "![barplot2](eda3.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "Again, similar to the other plot it's quite clear that there are some days where there tends to be more orders than others. For instance, in the graph above it's clear that most orders are on the first day, i.e. Monday.\n",
    "\n",
    "##### Most Popular Department\n",
    "\n",
    "Similarly, we can check and see which department is the most popular;\n",
    "\n",
    "<br />\n",
    "\n",
    "![barplot3](eda4.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "And again, it's quite clear which department is the most popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4878692",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Summary of data preprocessing steps made on original data (full code in other jupyter notebook (`Assignment_3_code.ipynb`)):\n",
    "- The column `days_since_prior_order` was the only column containing NaN values. Out of 2019501 rows, 124342 were `NaN`, meaning that the user had not previously ordered. For simplicity's sake, these values were filled with a value of -1.\n",
    "- Columns such as `order_id` and `user_id` that are mostly used for \"identification\" purposes are dropped, as it is thought that they play little role in our final machine learning model's performance.\n",
    "- Similarly, the `product_name` column was dropped, as the `product_id` column already exists.\n",
    "- In our dataset, we have two different categorical columns; `department` and `product_name`. Given that the latter column has been dropped, `department` will be converted into categorical codes.\n",
    "\n",
    "The final dataframe used in the following sections looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f07ab109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_number</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "      <th>department_id</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_number  order_dow  order_hour_of_day  days_since_prior_order   \n",
       "0             1          2                 18                    -1.0  \\\n",
       "1             1          2                 18                    -1.0   \n",
       "2             1          2                 18                    -1.0   \n",
       "3             1          2                 18                    -1.0   \n",
       "4             1          2                 18                    -1.0   \n",
       "\n",
       "   product_id  add_to_cart_order  reordered  department_id  department  \n",
       "0          17                  1          0             13          16  \n",
       "1          91                  2          0             16           7  \n",
       "2          36                  3          0             16           7  \n",
       "3          83                  4          0              4          19  \n",
       "4          83                  5          0              4          19  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/preprocessed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1fc00",
   "metadata": {},
   "source": [
    "### *Use K-means clustering to identify the optimal number of clusters. Experiment with different values of K and use metrics such as the elbow method and silhouette score to evaluate the performance of the clustering.*\n",
    "\n",
    "For implementing the algorithm, [Scikit-Learn's implementation of K-means clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) was used.\n",
    "\n",
    "Results using the elbow method (plotting WCSS against the value of k) shows that `k = 8` would be the most optimal value. Using the elbow method again (plotting silhouette score against the value of k) shows that `k=` would be the most optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c309cfa",
   "metadata": {},
   "source": [
    "### *Visualize the clusters and analyze their characteristics. This may involve plotting the clusters in 2D or 3D using PCA or t-SNE.*\n",
    "\n",
    "> For this, sklearn's implementation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) was used.\n",
    "\n",
    "For each number of k, we can reduce the number of features down to 2 for easy visualization using PCA, and then iterating over different values of k (from 2 to 10) to find the best (i.e. most well separated clusters).\n",
    "\n",
    "The results of which are shown below: \n",
    "\n",
    "<br />\n",
    "\n",
    "![subplot1](k_subplot.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "Where the number of colors in the legend corresponds to the number of clusters used. We're looking for the clusters that are best separated from one another, and it appears that when `k=5`, the clusters are best separated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847060a2",
   "metadata": {},
   "source": [
    "### *Experiment with other clustering algorithms such as DBSCAN or hierarchical clustering, and compare their performance with K-means.*\n",
    "\n",
    "The results are as follows:\n",
    "\n",
    "<br />\n",
    "\n",
    "![clustering](clustering.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "With hierarchical clustering having the best absolute score. However, with more fine tuning one might obtain better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83112f4",
   "metadata": {},
   "source": [
    "### \\**Try to reduce data dimensionality using PCA before training your model, use different numbers of components and report their effects.*\n",
    "\n",
    "The preprocessed dataset used in training previous k-means models consisted of 9 features. Therefore, \n",
    "\n",
    "The methodology I used for this was iterating over all possible number of features, calculating the principal components and transforming the data, and then finding the best number of clusters between 2 and 15, and taking the lowest silhouette score into account. The results are shown in the graph below;\n",
    "\n",
    "<br />\n",
    "\n",
    "![PCA + K-Means](pca_kmeans.png \"Code in Assignment_3_code.ipynb\")\n",
    "\n",
    "<br />\n",
    "\n",
    "The best results was when we had reduced our dataset down to 8 features with 14 clusters, however, due to computational complexity, opting for 5 features with 12 clusters might be the better choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
